{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b5b42c-2241-44f6-afa6-e11c86b1b43f",
   "metadata": {},
   "source": [
    "# GPT - Generative Pretrained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf9341-a811-4aa0-888d-c3e7ee220a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from IPython.display import display\n",
    "import sympy as sp\n",
    "import urllib\n",
    "sp.init_printing(use_latex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "#import os\n",
    "#os.environ['LD_LIBRARY_PATH']='/opt/conda/lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f310e7-63c0-4f63-9a27-aa6486676807",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TransformerBlock\n",
    "### Used for an encoder or typical classification/regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e4a28-57a8-414f-9080-740fcbb0815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                   key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd1c48-7619-4877-8659-cb56a761417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(GPTransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                   key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        y = x\n",
    "        y = self.layernorm1(y, training=training)\n",
    "        y = self.att(y, y,\n",
    "                     use_causal_mask = True,\n",
    "                     training=training)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x += y\n",
    "        y = x\n",
    "        y = self.layernorm2(y, training=training)\n",
    "        y = self.ffn(y, training=training)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        return x + y\n",
    "    # def call(self, inputs, training):\n",
    "    #     attn_output = self.att(inputs, inputs,\n",
    "    #                            use_causal_mask = True)\n",
    "    #     attn_output = self.dropout1(attn_output, training=training)\n",
    "    #     out1 = self.layernorm1(inputs + attn_output)\n",
    "    #     ffn_output = self.ffn(out1)\n",
    "    #     ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    #     return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcfd19-f301-4e00-aa96-c782f3a275fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source pulled from KerasNLP: https://github.com/keras-team/keras-nlp/blob/v0.4.1/keras_nlp/layers/sine_position_encoding.py#L22\n",
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding layer.\n",
    "    This layer calculates the position encoding as a mix of sine and cosine\n",
    "    functions with geometrically increasing wavelengths. Defined and formulized\n",
    "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "    Takes as input an embedded token tensor. The input must have shape\n",
    "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
    "    positional encoding the same size as the embedded token tensor, which\n",
    "    can be added directly to the embedded token tensor.\n",
    "    This layer optionally accepts `tf.RaggedTensor`s as inputs to process\n",
    "    batches of sequences of different lengths. The one ragged dimension must be\n",
    "    the dimension that corresponds to the sequence, that is, the penultimate\n",
    "    dimension.\n",
    "    Args:\n",
    "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
    "            curves, as described in Attention is All You Need. Defaults to\n",
    "            10000.\n",
    "    Examples:\n",
    "    ```python\n",
    "    # create a simple embedding layer with sinusoidal positional encoding\n",
    "    seq_len = 100\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 32\n",
    "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_dim\n",
    "    )(inputs)\n",
    "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
    "    outputs = embedding + positional_encoding\n",
    "    ```\n",
    "    References:\n",
    "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_wavelength=10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
    "        # with other layers.\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            bounding_shape = inputs.bounding_shape()\n",
    "            position_embeddings = (\n",
    "                self._compute_trim_and_broadcast_position_embeddings(\n",
    "                    bounding_shape,\n",
    "                )\n",
    "            )\n",
    "            # then apply row lengths to recreate the same ragged shape as inputs\n",
    "            return tf.RaggedTensor.from_tensor(\n",
    "                position_embeddings,\n",
    "                inputs.nested_row_lengths(),\n",
    "            )\n",
    "        else:\n",
    "            return self._compute_trim_and_broadcast_position_embeddings(\n",
    "                tf.shape(inputs),\n",
    "            )\n",
    "\n",
    "    def _compute_trim_and_broadcast_position_embeddings(self, shape):\n",
    "        seq_length = shape[-2]\n",
    "        hidden_size = shape[-1]\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "        # even indices are sine, odd are cosine\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        # embedding shape is [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        return tf.broadcast_to(positional_encodings, shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_wavelength\": self.max_wavelength,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2880b-a01e-4487-9288-b0abe5c5f9a5",
   "metadata": {},
   "source": [
    "## MaskedTokenAndPositionEmbedding\n",
    "### Converts integer tokens to embeddings and adds position random position encodings - masked since it's typically used with encoder-decoder (integer value 0 is special and will result in no embedding/position passed forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a37f3-8856-4d3d-8fae-07ee380d7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, input_dim, output_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "                                              output_dim=output_dim,\n",
    "                                              mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d1fa2-4274-4a47-ae4c-ba8bcb91d371",
   "metadata": {},
   "source": [
    "## MaskedTokenAndSinePositionEmbedding\n",
    "### Converts integer tokens to embeddings and adds sinusoid position encodings - masked since it's typically used with encoder-decoder (integer value 0 is special and will result in no embedding/position passed forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357a7f5-fe53-473a-a350-4c2171ec3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndSinePositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, max_wavelength=10000,**kwargs):\n",
    "        super(MaskedTokenAndSinePositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = SinePositionEncoding(max_wavelength=max_wavelength)\n",
    "\n",
    "    def call(self, x):\n",
    "        mask = tf.expand_dims(tf.sign(x),-1)\n",
    "        x = self.token_emb(x)\n",
    "        positions = self.pos_emb(x)\n",
    "        positions = positions * mask\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd5d9e-14d9-47d1-b395-48012a43bdfd",
   "metadata": {},
   "source": [
    "## Masked Loss and Accuracy\n",
    "### Needed to handle encoder-decoder with inputs of differing length (using masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3aaee9-4f60-4ae2-84bf-8cb6d985d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom masked loss/accuracy functions\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fadef-9a48-468a-bef5-a03032e28a66",
   "metadata": {},
   "source": [
    "### New Task: Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4565b27-8c70-4aa4-b0e7-ea5c80f7b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/Austen.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1a125-a004-4d1e-96b8-ca186ccd41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 'This is a test.'\n",
    "split = 2\n",
    "[temp[j:j+split] for j in range(0,len(temp),split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4d92d-f775-44b9-8389-049d125d9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 20\n",
    "model_length = 10*segment_size\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ac4db-2604-4359-92ae-8759234d6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "text = []\n",
    "j = 0\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i] == '':\n",
    "        line = ' '.join(lines[j:i])\n",
    "        if line != '':\n",
    "            text = text + [line[k:k+model_length-2] for k in range(0, len(line), model_length-2)]\n",
    "            # text = text + [line]\n",
    "        j = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20879b09-acb9-472f-8f58-b4d755662f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0f254-cfa9-4dab-a1e9-8dcc3e1c8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368d882-ad42-402a-aa1e-ae64f47daf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset? - All of the data will take some time...\n",
    "n_seq = len(text)\n",
    "n_seq = 1000\n",
    "split_point = int(n_seq * 0.8)\n",
    "text = text[:n_seq]\n",
    "np.random.shuffle(text) # In-place modification\n",
    "max_length = np.max([len(i) for i in text])+2 # Start+stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425dd40-b440-4594-bbc7-57b2b4d630d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49873b7-938c-4ddf-a2a8-3ceedaca1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "           [mapping[i] for i in list(x)] + \\\n",
    "           [mapping['<STOP>']] + \\\n",
    "           [0]*(max_length-len(list(x))-2)\n",
    "\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8913c4c-fef6-4a81-9235-2775284fb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for sentence in text for char in sentence})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e8ca0-2a48-4fa8-ad70-680116653dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2dad0-43cb-4969-a98e-d8ecb31f59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_to_i_pandp['U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068bdbc7-d418-4787-a30d-440b9d772842",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c_to_i_pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b467cd1-373f-4846-8792-6926e07fbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fa5fe-5e60-4aaa-ad51-9ec9f3944564",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = encode_seq(text[0],c_to_i_pandp,max_length)\n",
    "print(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039b597-8faf-4af6-8f8e-b41f179c3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_seq(temp,i_to_c_pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502ac39-6e14-4cbc-b625-9092d0794371",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62e38e-64d5-4627-9598-ebc9bfd4b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = encode_seq(text[1],c_to_i_pandp,max_length)\n",
    "print(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04bb1-41d4-44be-8cc1-349208688c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_seq(temp,i_to_c_pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deacfd7-2446-486e-b9ab-f8ded22801d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, max_length, batch_size=32, **kwargs):\n",
    "        super(DataGenerator, self).__init__(**kwargs)\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data # a handle only...\n",
    "        self.indices = np.arange(self.data[0].shape[0])\n",
    "        self.max_length = max_length\n",
    "        self.idx = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'The number of batches per epoch'\n",
    "        return int(np.floor(self.data[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one (enumerated) batch of data'\n",
    "        # Generate indices for a batch and grab batch\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return self.__data_generation(indices)\n",
    "\n",
    "    def __data_generation(self, ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Generate data\n",
    "        x = tf.convert_to_tensor(self.data[0][ids],dtype=tf.int32)\n",
    "        y = tf.convert_to_tensor(self.data[1][ids],dtype=tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        return np.random.shuffle(self.indices)\n",
    "\n",
    "    ## Needed for TF Dataset conversion...\n",
    "    def output_signature(self):\n",
    "        sig = self[0]\n",
    "        return (tf.TensorSpec.from_tensor(sig[0]),\n",
    "                tf.TensorSpec.from_tensor(sig[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1fc2e-4cb2-4dd4-9681-32dd827dee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length + (segment_size - (max_length-1) % segment_size)) for x in text])\n",
    "training = DataGenerator((X[:split_point,:-1],\n",
    "                          X[:split_point,1:]),model_length,batch_size)\n",
    "validation = DataGenerator((X[split_point:,:-1],\n",
    "                            X[split_point:,1:]),model_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa4ab2-bcab-4ff3-88ec-cd13e7d57f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c88aea-70fb-4772-8d12-4446520ed528",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292c589-d666-4c0c-a84b-e04a7f8de6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e22252-a960-4ac0-b952-0d51542c78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([len(i)+1 for i in temp])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6429e03-1358-4325-99af-7a3dcb3067a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.random.randint(i) for i in temp])-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b7600-a243-4fa4-b665-7ffa873797ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch = training[0]\n",
    "my_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b459d02-529a-4639-b8bd-f961784e4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790e292-624c-4c76-9dd0-093f6ed0ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2550e0a-d47d-4faf-b657-65288f09e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.output_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f97df0-2195-4b26-93ef-3b37ba46cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "training[0][0].shape\n",
    "training[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff76a16-c541-4cfe-88cc-d1158b437452",
   "metadata": {},
   "outputs": [],
   "source": [
    "training[7][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e2cea-277b-4998-b480-58b6cffc58ef",
   "metadata": {},
   "source": [
    "### Standard GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b83d3-e2b0-46b0-a2b4-12cc03b798ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(i_to_c_pandp)\n",
    "embedding_size = 128\n",
    "stack = 5\n",
    "num_heads = 12\n",
    "memory_size = segment_size * 3\n",
    "\n",
    "y = x = keras.layers.Input((None,))\n",
    "y = MaskedTokenAndSinePositionEmbedding(input_dim=n_tokens,\n",
    "                                        output_dim=embedding_size)(y)\n",
    "for _ in range(stack):\n",
    "    y = GPTransformerBlock(embedding_size,\n",
    "                           num_heads,\n",
    "                           embedding_size*2)(y)\n",
    "\n",
    "y = keras.layers.Dense(n_tokens)(y)\n",
    "\n",
    "model = keras.Model(x,y)\n",
    "model.compile(loss=MaskedSparseCategoricalCrossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=MaskedSparseCategoricalAccuracy)\n",
    "model.summary()\n",
    "keras.utils.plot_model(model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0640c11-d21d-4947-a7af-ddeaccb97260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"i_to_c_pandp\",\"wb\") as f:\n",
    "#     pickle.dump(i_to_c_pandp,f)\n",
    "# with open(\"c_to_i_pandp\",\"wb\") as f:\n",
    "#     pickle.dump(c_to_i_pandp,f)\n",
    "# model.save_weights(\"gpt-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb6197-add8-4718-992c-a93d540d3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"i_to_c_pandp\",\"rb\") as f:\n",
    "    pickle.load(f)\n",
    "with open(\"c_to_i_pandp\",\"rb\") as f:\n",
    "    pickle.load(f)\n",
    "model.load_weights(\"gpt-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fb754-9e55-43fc-8862-66f22c49d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "history = model.fit(training,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1412b1d-b4b6-40c6-a7fc-23a2d16af5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:',model.evaluate(training)[1]*100.0,'%')\n",
    "print('Validation Accuracy:',model.evaluate(validation)[1]*100.0,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba5e03-7bba-4c5a-bc39-e07c7a2494ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)  \n",
    "# summarize history for accuracy \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['MaskedSparseCategoricalAccuracy'])  \n",
    "plt.plot(history.history['val_MaskedSparseCategoricalAccuracy'])  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training','Validation'],loc='lower right')\n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['Training','Validation'],loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ba83a-39ce-493f-8f23-2d84ccc864e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbd8eb-e7d2-4f6e-b83f-98f5ed98ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998894b5-d865-4831-9654-3a4d1b416716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should we see?\n",
    "i = 0\n",
    "print('Input:', training[0][0][i])\n",
    "print('Output:', training[0][1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9da76d-ff7b-47cc-a485-2776289834ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input:', decode_seq(training[0][0][i],i_to_c_pandp))\n",
    "print('Target:', decode_seq(training[0][1][i],i_to_c_pandp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15ff6c-5d53-4aef-a65c-cb6ee1d2bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-forcing\n",
    "result = model(training[0][0][i:i+1]).numpy()\n",
    "result.argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36113b-4d84-48f9-ba09-e724478051c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_seq(result.argmax(-1)[0],i_to_c_pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3603dc-4c2e-46fa-a81d-5a75efcbca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much agreement really? - remember masked tokens (0) don't count\n",
    "MaskedSparseCategoricalAccuracy(training[0][1][i:i+1],result).numpy()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec55ab-043d-4319-8ce8-552ab3f700f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-forcing\n",
    "result = model(validation[0][0][i:i+1]).numpy()\n",
    "result.argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988d46c-d7b9-4ae9-b698-09f3bb0330e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much agreement really? - remember masked tokens (0) don't count\n",
    "MaskedSparseCategoricalAccuracy(validation[0][1][i:i+1],result).numpy()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecf1c8-0d42-4794-b393-fe0d9e0750d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_token(x):\n",
    "    x = np.cumsum(x)\n",
    "    return np.argmax(x > np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f862e-794b-46a5-8a92-88f2aefd6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 1\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 16\n",
    "tokens = np.zeros(data.shape,dtype=np.int32)\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 2:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],i_to_c_pandp),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],i_to_c_pandp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8b7e6-c003-4fb4-a24e-344382b73b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 2\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 16\n",
    "tokens = np.zeros(data.shape,dtype=np.int32)\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 2:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],i_to_c_pandp),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],i_to_c_pandp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aca358-c113-4353-a5f8-3271024474db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 0\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 16\n",
    "tokens = np.zeros(data.shape,dtype=np.int32)\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 2:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],i_to_c_pandp),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],i_to_c_pandp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f61d1e-24d9-4b2a-a946-505933f68447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 0\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 16\n",
    "tokens = np.zeros(data.shape,dtype=np.int32)\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 2:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],i_to_c_pandp),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Final:\")\n",
    "print(decode_seq(tokens[0],i_to_c_pandp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b922a-a350-4673-b5b0-c3654a91247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023de6ed-41bf-44b4-a60d-c909fcef6785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
