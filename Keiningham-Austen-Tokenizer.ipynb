{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c678fa6e-8869-4f54-a184-291ed4ba99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import GPT2Tokenizer to use\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce71511-476e-4614-b0fe-b2028bb9155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 07:12:37.852691: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from IPython.display import display\n",
    "import sympy as sp\n",
    "import urllib\n",
    "sp.init_printing(use_latex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "#import os\n",
    "#os.environ['LD_LIBRARY_PATH']='/opt/conda/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4bf688-56a2-4abf-b8e5-d8464dccea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transormer block setup\n",
    "#Initializes multi-head attention, feed-forward network, layer normalization\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                   key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"), #feed-forward\n",
    "             keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70993c4e-2095-41e1-9c58-f01f851beb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize transformer, multi-head-attention, ff net, and mask\n",
    "class GPTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(GPTransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                   key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        y = x\n",
    "        y = self.layernorm1(y, training=training)\n",
    "        y = self.att(y, y,\n",
    "                     use_causal_mask = True,\n",
    "                     training=training)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x += y\n",
    "        y = x\n",
    "        y = self.layernorm2(y, training=training)\n",
    "        y = self.ffn(y, training=training)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0597b470-1e2d-4d9f-86f5-669df0101937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional encoding for self-attention\n",
    "\n",
    "# Source pulled from KerasNLP: https://github.com/keras-team/keras-nlp/blob/v0.4.1/keras_nlp/layers/sine_position_encoding.py#L22\n",
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding layer.\n",
    "    This layer calculates the position encoding as a mix of sine and cosine\n",
    "    functions with geometrically increasing wavelengths. Defined and formulized\n",
    "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "    Takes as input an embedded token tensor. The input must have shape\n",
    "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
    "    positional encoding the same size as the embedded token tensor, which\n",
    "    can be added directly to the embedded token tensor.\n",
    "    This layer optionally accepts `tf.RaggedTensor`s as inputs to process\n",
    "    batches of sequences of different lengths. The one ragged dimension must be\n",
    "    the dimension that corresponds to the sequence, that is, the penultimate\n",
    "    dimension.\n",
    "    Args:\n",
    "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
    "            curves, as described in Attention is All You Need. Defaults to\n",
    "            10000.\n",
    "    Examples:\n",
    "    ```python\n",
    "    # create a simple embedding layer with sinusoidal positional encoding\n",
    "    seq_len = 100\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 32\n",
    "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_dim\n",
    "    )(inputs)\n",
    "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
    "    outputs = embedding + positional_encoding\n",
    "    ```\n",
    "    References:\n",
    "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_wavelength=10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
    "        # with other layers.\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            bounding_shape = inputs.bounding_shape()\n",
    "            position_embeddings = (\n",
    "                self._compute_trim_and_broadcast_position_embeddings(\n",
    "                    bounding_shape,\n",
    "                )\n",
    "            )\n",
    "            # then apply row lengths to recreate the same ragged shape as inputs\n",
    "            return tf.RaggedTensor.from_tensor(\n",
    "                position_embeddings,\n",
    "                inputs.nested_row_lengths(),\n",
    "            )\n",
    "        else:\n",
    "            return self._compute_trim_and_broadcast_position_embeddings(\n",
    "                tf.shape(inputs),\n",
    "            )\n",
    "\n",
    "    def _compute_trim_and_broadcast_position_embeddings(self, shape):\n",
    "        seq_length = shape[-2]\n",
    "        hidden_size = shape[-1]\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "        # even indices are sine, odd are cosine\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        # embedding shape is [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        return tf.broadcast_to(positional_encodings, shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_wavelength\": self.max_wavelength,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551fe920-63e0-4b15-b1af-3a3f466ba419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token and position embedding to allow the model to differentiate token positions, for self-attention\n",
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, input_dim, output_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "                                              output_dim=output_dim,\n",
    "                                              mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c07eef-d0a6-43d9-ae35-22f4c73a3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sine position embeddings, for self-attention\n",
    "class MaskedTokenAndSinePositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, max_wavelength=10000,**kwargs):\n",
    "        super(MaskedTokenAndSinePositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = SinePositionEncoding(max_wavelength=max_wavelength)\n",
    "\n",
    "    def call(self, x):\n",
    "        mask = tf.expand_dims(tf.sign(x),-1)\n",
    "        x = self.token_emb(x)\n",
    "        positions = self.pos_emb(x)\n",
    "        positions = positions * mask\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d6397c9-897a-4e3e-afc9-862aa9dd1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom masked loss/accuracy functions\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67db5ca-183c-496d-b4fd-f0386e7f87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-19 07:12:46--  https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/Austen.txt\n",
      "Resolving www.cs.mtsu.edu (www.cs.mtsu.edu)... 161.45.162.100\n",
      "Connecting to www.cs.mtsu.edu (www.cs.mtsu.edu)|161.45.162.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4340030 (4.1M) [text/plain]\n",
      "Saving to: ‘Austen.txt’\n",
      "\n",
      "Austen.txt          100%[===================>]   4.14M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-06-19 07:12:46 (102 MB/s) - ‘Austen.txt’ saved [4340030/4340030]\n",
      "\n"
     ]
    }
   ],
   "source": [
    " !wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/Austen.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b540cd38-8382-4e0a-9eaa-003bf66d1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 20\n",
    "model_length = 10*segment_size\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "261339d7-2004-486d-8816-473cff3b1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "text = []\n",
    "j = 0\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i] == '':\n",
    "        line = ' '.join(lines[j:i])\n",
    "        if line != '':\n",
    "            text = text + [line[k:k+model_length-2] for k in range(0, len(line), model_length-2)]\n",
    "            # text = text + [line]\n",
    "        j = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b519a8b-584e-4ef7-bebf-3f57d96251f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAPCAYAAACm25zCAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACsklEQVRIDb2W0VEcMQyG9zJXwJESkg4IdHDpIEwqCHQQJk93bxnSASkh0EHogEAHUAKhg8v3mbXH3tMu+xTN7EmWfv22Zdmw2O123RzZbrfvwK35fmE/z8n5n5iFG+kXed5PfIR+4jvHf58Xg/0J+yqPA/0M5iDwd/gP8Z+hz+o4Y4uTfStsxxf4b2pcZA85lz3ZJfpjTsC+wL7Tx5dJj/FpP2ZcpT2pXIjKXUwLUIqiF14XbrHyRvRZrN/oE75rcRPScC4BuuhCZiIkTnCKKbhUGV/ZrDgFn1VUhxPj/2o8EPlPibvwnJuL9o1Y9u2lRpxvQFnNB4JWqBZJV/jTQrFv62Bl2wpNIXIMvy3lffIbiifUxMBHuCZvjNONpHaZIFnJRHyvQvg8ze/GR+QzmJ9RDP8N34E6x7FtLeXyRYW/IeeS5JMQ3nVWsyPe9HbG4vekDtHh3cBvS00tKFMlDd7OSG2OPbb5UU7vyJ5A5CZcaLjIPsFJ/faEfHN9xaKHocH3c7kJHxOL9qcB9IPXOMONkOslvyb5xwTpmvjYafrUThWh0IJz8enUsW0tX8vo1Zrk9I40Aont8ChZE2gHXu6w2uT5Gs1uqZqWXO+hF/4KO91N43M4m430CW/Re8+shJVYuadqnEzybClfunCTNR6M9yvdw9qPnVvLduvAzOIsrUWCi3uPLifRk3TosjBsKyV5agd0LfqPwdiataQ71/s9bdvuTgC2L5enMCazONNGIHIiFzDsazc3fEGO+hmjE/EpLc9pj+vg/asfXYrE2MXrG24i8yce4rM4lwDdsRWUdNjba3zDC++JKMMFvHjjX3NyXkYMi9Yxl4UT58V+jb/hXGw2mwcS3Uwk9xB+qAOMxdoWX7D3/kgOsBZGfOp3tPhb8lJx0PrrUxI7+U8jOSHnPwpOOBfHjjE8AAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle 27434$"
      ],
      "text/plain": [
       "27434"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a92df22-3081-44b9-9d9a-4371d6f1f3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd73e706-3f11-401e-af31-0eb2ded93ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset? - All of the data will take some time...\n",
    "n_seq = len(text)\n",
    "n_seq = 1000\n",
    "split_point = int(n_seq * 0.8)\n",
    "text = text[:n_seq]\n",
    "np.random.shuffle(text) # In-place modification\n",
    "max_length = np.max([len(i) for i in text])+2 # Start+stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "133b45aa-bc58-4293-8930-bd87f072718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode / Decode tokens using built-in functions\n",
    "#Using Parker's filter\n",
    "def remove_filler(val):\n",
    "    if val!=(len(tokenizer)-1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def encode_seq(x, tokenizer, max_length=0):\n",
    "    #String to token IDs\n",
    "    #Using \"0\" as padded token\n",
    "    encoded = tokenizer.encode(x, max_length=max_length, truncation=True)\n",
    "    padding = encoded + [len(tokenizer)-1 for i in range (max_length - len(encoded))]\n",
    "    return padding\n",
    "\n",
    "def decode_seq(x, tokenizer):\n",
    "    #Token IDs to string\n",
    "    filtered_text =filter(remove_filler,x)\n",
    "    filler_removed = list(filtered_text)\n",
    "    return tokenizer.decode(filler_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a1324d-7317-40fc-a716-4cc111ab27d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2061 3297 286 2045 582 318 1770 13 5780 1701 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n"
     ]
    }
   ],
   "source": [
    "temp = encode_seq(text[0], tokenizer, max_length)\n",
    "print(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "babe7ed4-ae14-43f4-b8aa-e99fb1ca13cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB0AAAAOCAYAAADT0Rc6AAAACXBIWXMAAA7EAAAOxAGVKw4bAAABn0lEQVQ4EY2U7U0CURBFF0MBBDqADlAqEDrQFrQD+Qn/DHZADdKBdmCkA7ACDR3gOevO5gHr7k4ymXn33Xn3fXeOx2O2XC6HWZbNce0G/8Hn4FuB1MBWRfubOMJXYPuUY17H6ywWCwXXkGZRWBQ80Z6Rvyf4J/kz2EaM2COIySuFyWt5VxQ480e8NIpc9QF/DRDsgbxHzAXFyeXYXtvWwBp5ik7xHWRnnZorVMSd0O7xi+0G+8CnSX0jT1EH31PkrKssJuPkPOtzi221X2vkdRFzZlU2FqR/i4dwFS+wflueK70wihV0W+NG9wvSf7thtxNrxasUpdgLtEH8hdjWBi2JgwtRhLyJnnG67VVnGRqxOt9tK96JKEJed8+mfLOOTDu2tepsA0svY2CWhwW2L0UZ+I7eEbFcIflQL6q85ZHHQMZYaXwijbxclIHHFE+IcXFiUCcSW+Y5+0We2zWAN/xQdDTy4ht8oyBmmg7qo/d/zY18R+KfnH6DX2C3YNs/Vn4ctbwuRAXdNs/z3MqBig5X5Qc/IXpxjCeCtLVa3i/mD735UAfpnQAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$\\displaystyle 200$"
      ],
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9031ef85-c9ae-4899-a0c2-a961e0dceeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"What sort of looking man is Mr. Martin?\"\n"
     ]
    }
   ],
   "source": [
    "decoded_temp = decode_seq(temp, tokenizer)\n",
    "print(decoded_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92016fc9-12da-465c-9229-11ae5f8152ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, max_length, batch_size=32, **kwargs):\n",
    "        super(DataGenerator, self).__init__(**kwargs)\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data # a handle only...\n",
    "        self.indices = np.arange(self.data[0].shape[0])\n",
    "        self.max_length = max_length\n",
    "        self.idx = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'The number of batches per epoch'\n",
    "        return int(np.floor(self.data[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one (enumerated) batch of data'\n",
    "        # Generate indices for a batch and grab batch\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return self.__data_generation(indices)\n",
    "\n",
    "    def __data_generation(self, ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Generate data\n",
    "        x = tf.convert_to_tensor(self.data[0][ids],dtype=tf.int32)\n",
    "        y = tf.convert_to_tensor(self.data[1][ids],dtype=tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        return np.random.shuffle(self.indices)\n",
    "\n",
    "    ## Needed for TF Dataset conversion...\n",
    "    def output_signature(self):\n",
    "        sig = self[0]\n",
    "        return (tf.TensorSpec.from_tensor(sig[0]),\n",
    "                tf.TensorSpec.from_tensor(sig[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f68a2c-59ee-4ef7-a9e5-fc76fd5a1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode in batches\n",
    "X = np.vstack([encode_seq(x,tokenizer,max_length + (segment_size - (max_length-1) % segment_size)) for x in text])\n",
    "training = DataGenerator((X[:split_point,:-1],\n",
    "                          X[:split_point,1:]),model_length,batch_size)\n",
    "validation = DataGenerator((X[split_point:,:-1],\n",
    "                            X[split_point:,1:]),model_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d725a0c-d64d-444f-ab7f-f62972001c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAoAAAAOCAYAAAAWo42rAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA+UlEQVQoFW2SsXHCQBBFD2jALdhOiXADCnAHogSbEhgiKXUJuAW5AxMo95gO7NQZ0IF4b+E00oz/zGrv7/z7u3enSdd1SdR1fUfaBrl+5A31fVCFRlVVu7zOmVpDlPKpana9kr5j5/jzAo0uIYQ8Es9jzZhl4RflEudPwtky3ljsJBP7CwQNqQyS0oZsFzd+WMuOClfwd4tApyVxkIihULczoZNX8kD8YBBdojXEUz+R1+TATeA4br7PjrZyrh4InU13D7ec3RbU6+GrUE6pKIpT27Zzln9TBFqfyc70H3Tc59ae2He12APu7F7R7/AeFxRtf+yVg5/iAl5gbKb5R2kWAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle 8$"
      ],
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268033c5-de99-49b5-a6c4-5d985aa7e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201,\n",
       "       201, 201, 201, 201, 201])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_length = len(temp)\n",
    "temp = np.array([temp_length+1 for i in temp])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef42fa26-6fb8-48f2-826f-c3955345c703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  64,  -17,   81,  -46,  -38,   36,   49, -100,  -66,   79,   86,\n",
       "        -84,   80,  -99,  -38,  -51,  -33, -100,   63,  -96,   93,   91,\n",
       "         98,  -92,   91,  -87,   -8,  -71,  -63,   51,   84,  -53,  -80,\n",
       "          4,   55,   41,   20,   68,  -55,   37,   80,  100,  -19,    7,\n",
       "         72,  -87,  -73,   28,    4,  -31,  -26,   40,   75,   96,  -15,\n",
       "        -78,    3,  -65,   45,  -70,   81,    0,  -41,  -19,   52,   87,\n",
       "        -18,  -31,  -70,    9,  -30,  -22,   64,  -82,  -21,   53,  -17,\n",
       "        -79,  -66,  -82,   64,    7,   18,    1,  -62,   55,   87,  -28,\n",
       "        -18,   23,   88,  -71,   32,  -34,   52,   19,   31,   82,   34,\n",
       "         80,   34,    2,   13,   45,    7,   73,   -8,   56,   54,   91,\n",
       "         -3,  -83,  -56,   36,   72,  -87,   77,  -63,   54,    9,  -31,\n",
       "         14,    3,   56,  -81,   57,   80,  -56,   73,   19,   68,   33,\n",
       "         79,   -3,  -53,   49,  -87,  -30,  -10,   23,  -23,   36,    0,\n",
       "        -58,  -54,   94,   31,   38,   55,   -5,    8,   33,  -78,  -58,\n",
       "         55,   96,  -31,  -65,   40,   57,    8,   76,   34,   22,   54,\n",
       "         63,  -75,   36,   40,  -78,  -19,   43,   76,   20,   41,   62,\n",
       "         51,   89,   26,   31,  -56,    3,   51,  -38,   33,   68,   34,\n",
       "         64,  -24,   96,   27,   71,   29,   16,  -34,   62,    5,  -78,\n",
       "         57,  -90])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.random.randint(i) for i in temp])-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32957d5c-4627-41ed-9e6d-e41a7d7eab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 07:12:59.021991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-19 07:12:59.022038: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-19 07:12:59.022073: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-cwk2p): /proc/driver/nvidia/version does not exist\n",
      "2023-06-19 07:12:59.022587: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 200])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch = training[0]\n",
    "my_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2adcec-fd5d-4339-8805-61d991088907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalize and compile model\n",
    "n_tokens = len(tokenizer)\n",
    "embedding_size = 128\n",
    "stack = 5\n",
    "num_heads = 12\n",
    "memory_size = segment_size * 3\n",
    "\n",
    "y = x = keras.layers.Input((None,))\n",
    "y = MaskedTokenAndSinePositionEmbedding(input_dim=n_tokens,\n",
    "                                        output_dim=embedding_size)(y)\n",
    "for _ in range(stack):\n",
    "    y = GPTransformerBlock(embedding_size,\n",
    "                           num_heads,\n",
    "                           embedding_size*2)(y)\n",
    "\n",
    "y = keras.layers.Dense(n_tokens)(y)\n",
    "\n",
    "model = keras.Model(x,y)\n",
    "#model.compile(loss=MaskedSparseCategoricalCrossentropy,\n",
    "#              optimizer=keras.optimizers.Adam(),\n",
    "#              metrics=MaskedSparseCategoricalAccuracy)\n",
    "#model.summary()\n",
    "#keras.utils.plot_model(model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8fc7666-238a-461b-b9a7-d7a624724ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#started at 8:00pm 6/15\n",
    "#server disconnect at ~5:00am\n",
    "#reconnect at 7:00am \n",
    "#finished at 9:10am 6/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6726dbf-c951-4a69-8b44-ccc2368efd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  196M  100  196M    0     0  78.4M      0  0:00:02  0:00:02 --:--:-- 78.4M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://s-stem-data.nyc3.digitaloceanspaces.com/Keiningham-Weights/Keiningham-Weights-2.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3986ed56-45ee-420e-826e-09adbe93ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf Keiningham-Weights-2.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ff3f08-c7a3-46ff-bee4-926001c280c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f41aed22fb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"Keiningham-Weights-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2fc5a4-fad6-4ce6-923e-d37e105de408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tf.Tensor(\n",
      "[    1  5297    11 20461    64    11   340   318  3194   503   287   674\n",
      "  1218  2443    13   775 18984   340   422   262   412  1455   415 29677\n",
      "    82    13   632   373 46619   624   338    11   345   760   526 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256], shape=(200,), dtype=int32)\n",
      "Output: tf.Tensor(\n",
      "[ 5297    11 20461    64    11   340   318  3194   503   287   674  1218\n",
      "  2443    13   775 18984   340   422   262   412  1455   415 29677    82\n",
      "    13   632   373 46619   624   338    11   345   760   526 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256], shape=(200,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# What should we see?\n",
    "i = 0\n",
    "print('Input:', training[0][0][i])\n",
    "print('Output:', training[0][1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a46483-4524-43ab-92cb-6c763c8504df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   40,    11, 20461,    64,    11,   340,   318,  3194,   503,\n",
       "         287,   674,  1218,  2443,    13,   775, 18984,   340,   422,\n",
       "         262,   412,  1455,   415, 29677,    82,    13,   632,   373,\n",
       "       46619,   624,   338,    11,   345,   760,   526, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "       50256, 50256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teacher-forcing\n",
    "result = model(training[0][0][i:i+1]).numpy()\n",
    "result.argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "699df111-02c0-46e4-91e4-db78c357878b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I, papa, it is written out in our second page. We copied it from the Elegant Extracts. It was Garrick\\'s, you know.\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(result.argmax(-1)[0],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1224485d-9eec-449a-84e8-f5b6cbdf50ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 99.5000004768372$"
      ],
      "text/plain": [
       "99.50000047683716"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much agreement really? - remember masked tokens (0) don't count\n",
    "MaskedSparseCategoricalAccuracy(training[0][1][i:i+1],result).numpy()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e26c7db-61b2-4369-a347-88d8615fc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_token(x):\n",
    "    x = np.cumsum(x)\n",
    "    return np.argmax(x > np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "184c8ae8-8738-46af-9803-4885c7848632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\"So do I,\" said Mrs. Weston gently, \"very much.\"\n",
      "\n",
      "Prompt:\n",
      "\"So do I,\" said\n",
      "\n",
      "Decoding:\n",
      "\"So do I,\" said Mrs. Weston gently, \"very much.\"\n",
      "\n",
      "Remodeled:\n",
      "I do I,\" said Mrs. Weston gently, \"very much.\"\n"
     ]
    }
   ],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 1\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 6\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=50256) #Courtesy of Parker Daniels \n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == len(tokenizer)-1:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70fc1916-065b-43fa-8137-c3492cdab0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "n.\n",
      "\n",
      "Prompt:\n",
      "n.\n",
      "\n",
      "Decoding:\n",
      "n.\n",
      "\n",
      "Remodeled:\n",
      "f He\n"
     ]
    }
   ],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 3\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 4\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=50256) #Courtesy of Parker Daniels\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == len(tokenizer)-1:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2928eb0b-0aca-4952-8972-b0cc449374d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "It is a truth univ\n",
      "\n",
      "Prompt:\n",
      "It is a truth un\n",
      "\n",
      "Decoding:\n",
      "It is a truth un just kind about hear to be able with take all the respectable; with me at a better than than to have said. She can be in your name, often himself to suppose herself, as a man,\n",
      "\n",
      "Remodeled:\n",
      " was only very Emma just kind for hear to be able to take all the respectable; for me at present narrow than than have could said at I can be in any name's often himself, be you, as a man, s\n"
     ]
    }
   ],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "sample = \"It is a truth universally acknowledged\"\n",
    "data = np.array(encode_seq(sample, tokenizer, max_length))\n",
    "data = data.reshape(1,-1)\n",
    "prompt = len(sample.split())\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=len(tokenizer)-1) #Courtesy of Parker Daniels\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == len(tokenizer)-1:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72e51bbd-3ed8-4c91-a054-65a32f100f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems to have memorized the original text\n",
    "#The model does suffer for short prompts\n",
    "#Custom prompts are interesting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
