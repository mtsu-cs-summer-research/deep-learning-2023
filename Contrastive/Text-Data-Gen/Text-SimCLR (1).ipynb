{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d642339-4d90-41af-9c35-d390952b976b",
   "metadata": {},
   "source": [
    "### SimCLR process used on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddc956-3ed9-43ab-8631-cfc16779c31d",
   "metadata": {},
   "source": [
    "Process:\n",
    "\n",
    "- New data generator\n",
    "- Causal Mask\n",
    "- Troubleshoot when it breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec3a7c82-d1d3-43d2-8ac3-4bfe5ebef704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "\n",
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"]='/opt/conda/lib'\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/opt/conda/pkgs/cuda-toolkit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d192f20e-8a18-45e0-8417-a5e10ef3563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948c898d-0338-4990-8215-e1d2771055c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Austen Text\n",
    "# !wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/Austen.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d33c1-785d-4ba7-97e7-1d753447a2d9",
   "metadata": {},
   "source": [
    "### Setup Austen Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4410e507-edb9-4481-a78a-9a4549b5ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 20\n",
    "model_length = 10*segment_size\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc358407-da0d-4ba1-a020-619836b3a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "text = []\n",
    "j = 0\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i] == '':\n",
    "        line = ' '.join(lines[j:i])\n",
    "        if line != '':\n",
    "            text = text + [line[k:k+model_length-2] for k in range(0, len(line), model_length-2)]\n",
    "            # text = text + [line]\n",
    "        j = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2582e256-1e69-42dc-afb3-2316087f9a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4211a3e-1694-46bb-88fe-8788301837a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4117c48-321f-40ff-a272-65444eb048f8",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "- Create batches of data based on encoded text\n",
    "- Indexing will pull randomly cropped sections text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec10f184-4bb6-4d94-acbe-a547e52e2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import GPT2Tokenizer to use\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "304466c1-5dc5-4761-9ab7-8c34f8584e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode / Decode tokens using built-in functions\n",
    "\n",
    "def encode_seq(x, max_length=0):\n",
    "    #String to token IDs\n",
    "    #Using end of text token as padding\n",
    "    encoded = tokenizer.encode(x, max_length=max_length, truncation=True)\n",
    "    padding = encoded + [50256 for i in range (max_length - len(encoded))]\n",
    "    return padding\n",
    "\n",
    "def decode_seq(x):\n",
    "    #Token IDs to string\n",
    "    remove_padding = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 50256: #end of text\n",
    "            break\n",
    "        remove_padding.append(x[i])\n",
    "    return tokenizer.decode(remove_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d8f8ce2-5161-4ea2-94d9-cdff736a8cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world\n"
     ]
    }
   ],
   "source": [
    "test = text[0]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f51b976e-c846-4a05-ad83-06270a142a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the world\n"
     ]
    }
   ],
   "source": [
    "def random_crop(text):\n",
    "    crop_percent = np.random.uniform(0.4, 1)\n",
    "\n",
    "    if crop_percent == 1:\n",
    "        return text\n",
    "        \n",
    "    length = len(text)\n",
    "    index = int(np.floor(length*crop_percent))\n",
    "\n",
    "    cropped_text = text[index:]\n",
    "    return cropped_text\n",
    "\n",
    "cropped_test = random_crop(test)\n",
    "print(cropped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd203256-0e23-41ba-a151-de53a913a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, max_length, batch_size=32, **kwargs):\n",
    "        super(DataGenerator, self).__init__(**kwargs)\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data # a handle only...\n",
    "        self.indices = np.arange(self.data[0].shape[0])\n",
    "        self.max_length = max_length\n",
    "        self.idx = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'The number of batches per epoch'\n",
    "        return int(np.floor(self.data[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one (enumerated) batch of data'\n",
    "        # Generate indices for a batch and grab batch\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return self.__data_generation(indices)\n",
    "\n",
    "    def __data_generation(self, ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Generate data\n",
    "        x = tf.convert_to_tensor(self.data[0][ids],dtype=tf.int32)\n",
    "        y = tf.convert_to_tensor(self.data[1][ids],dtype=tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        return np.random.shuffle(self.indices)\n",
    "\n",
    "    ## Needed for TF Dataset conversion...\n",
    "    def output_signature(self):\n",
    "        sig = self[0]\n",
    "        return (tf.TensorSpec.from_tensor(sig[0]),\n",
    "                tf.TensorSpec.from_tensor(sig[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f12b5de-25a5-4362-9deb-c9ce769d4051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset? - All of the data will take some time...\n",
    "n_seq = len(text)\n",
    "n_seq = 1000\n",
    "split_point = int(n_seq * 0.8)\n",
    "text = text[:n_seq]\n",
    "np.random.shuffle(text) # In-place modification\n",
    "max_length = np.max([len(i) for i in text])+2 # Start+stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6c72c8e-256e-4964-814e-100753380f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode in batches\n",
    "X = np.vstack([encode_seq(x,max_length + (segment_size - (max_length-1) % segment_size)) for x in text])\n",
    "training = DataGenerator((X[:split_point,:-1],\n",
    "                          X[:split_point,1:]),model_length,batch_size)\n",
    "validation = DataGenerator((X[split_point:,:-1],\n",
    "                            X[split_point:,1:]),model_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a84762eb-324b-4fb1-b267-6eeaa8972555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he table, took it up, and examined it very attentively. With the view of passing off an awkward moment, Emma smilingly said,'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_seq(training[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b63ee0-2708-4521-9127-783f171df4d8",
   "metadata": {},
   "source": [
    "### Transformer Setup\n",
    "- Causal Mask\n",
    "- Feed Forward\n",
    "- Relative Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "661d8fb1-f10c-4da5-a756-0f8728226a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Mutli Head Attention from SirDavidLudwig's DeepDNA\n",
    "# https://github.com/DLii-Research/deep-dna/tree/master\n",
    "\n",
    "class RelativeMultiHeadAttention(keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, max_seq_len=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._max_seq_len = max_seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self._max_seq_len is None:\n",
    "            self._max_seq_len = input_shape[1]\n",
    "            assert self._max_seq_len is not None, \"RelativeMultiHeadAttention requires max_seq_len to be specified.\"\n",
    "        self._rel_embeds = self.add_weight(\"relative_embeddings\",\n",
    "                                           shape=(self._max_seq_len, self._key_dim),\n",
    "                                           initializer=\"glorot_uniform\", trainable=True)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_seq_len\": self._max_seq_len\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def _skew(self, QEr):\n",
    "        padded = tf.pad(QEr, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "        shape = tf.shape(padded)\n",
    "        reshaped = tf.reshape(padded, (shape[0], shape[1], shape[3], shape[2]))\n",
    "        return reshaped[:,:,1:,:]\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        # Note: Applying scalar multiply at the smaller end of einsum improves\n",
    "        # XLA performance, but may introduce slight numeric differences in\n",
    "        # the Transformer attention head.\n",
    "        query = tf.multiply(query, 1.0 / np.sqrt(float(self._key_dim)))\n",
    "\n",
    "        # Compute relative position encodings\n",
    "        rel_enc = self._skew(tf.einsum(\"acbd,ed->abce\", query, self._rel_embeds))\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "        # attention scores.\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores + rel_enc, attention_mask)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_scores_dropout = self._dropout_layer(attention_scores, training=training)\n",
    "\n",
    "        # `context_layer` = [B, T, N, H]\n",
    "        attention_output = tf.einsum(self._combine_equation, attention_scores_dropout, value)\n",
    "        return attention_output, attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ca591f7-5ae7-4185-9332-fb58adea3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, max_seq_len=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = RelativeMultiHeadAttention(max_seq_len=max_seq_len, num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        y = x\n",
    "        y = self.layernorm1(y, training=training)\n",
    "        y, scores = self.att(y, y, return_attention_scores=True, training=training, use_causal_mask=True)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x += y\n",
    "        y = x\n",
    "        y = self.layernorm2(y, training=training)\n",
    "        y = self.ffn(y, training=training)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        return (x + y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "304654e8-b814-49b2-9483-4cd500783511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding for self-attention\n",
    "\n",
    "# Source pulled from KerasNLP: https://github.com/keras-team/keras-nlp/blob/v0.4.1/keras_nlp/layers/sine_position_encoding.py#L22\n",
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding layer.\n",
    "    This layer calculates the position encoding as a mix of sine and cosine\n",
    "    functions with geometrically increasing wavelengths. Defined and formulized\n",
    "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "    Takes as input an embedded token tensor. The input must have shape\n",
    "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
    "    positional encoding the same size as the embedded token tensor, which\n",
    "    can be added directly to the embedded token tensor.\n",
    "    This layer optionally accepts `tf.RaggedTensor`s as inputs to process\n",
    "    batches of sequences of different lengths. The one ragged dimension must be\n",
    "    the dimension that corresponds to the sequence, that is, the penultimate\n",
    "    dimension.\n",
    "    Args:\n",
    "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
    "            curves, as described in Attention is All You Need. Defaults to\n",
    "            10000.\n",
    "    Examples:\n",
    "    ```python\n",
    "    # create a simple embedding layer with sinusoidal positional encoding\n",
    "    seq_len = 100\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 32\n",
    "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_dim\n",
    "    )(inputs)\n",
    "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
    "    outputs = embedding + positional_encoding\n",
    "    ```\n",
    "    References:\n",
    "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_wavelength=10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
    "        # with other layers.\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            bounding_shape = inputs.bounding_shape()\n",
    "            position_embeddings = (\n",
    "                self._compute_trim_and_broadcast_position_embeddings(\n",
    "                    bounding_shape,\n",
    "                )\n",
    "            )\n",
    "            # then apply row lengths to recreate the same ragged shape as inputs\n",
    "            return tf.RaggedTensor.from_tensor(\n",
    "                position_embeddings,\n",
    "                inputs.nested_row_lengths(),\n",
    "            )\n",
    "        else:\n",
    "            return self._compute_trim_and_broadcast_position_embeddings(\n",
    "                tf.shape(inputs),\n",
    "            )\n",
    "\n",
    "    def _compute_trim_and_broadcast_position_embeddings(self, shape):\n",
    "        seq_length = shape[-2]\n",
    "        hidden_size = shape[-1]\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "        # even indices are sine, odd are cosine\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        # embedding shape is [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        return tf.broadcast_to(positional_encodings, shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_wavelength\": self.max_wavelength,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89928077-65d5-4b59-b1a0-72a3bdf67ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sine position embeddings, for self-attention\n",
    "class MaskedTokenAndSinePositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, max_wavelength=10000,**kwargs):\n",
    "        super(MaskedTokenAndSinePositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = SinePositionEncoding(max_wavelength=max_wavelength)\n",
    "\n",
    "    def call(self, x):\n",
    "        mask = tf.expand_dims(tf.sign(x),-1)\n",
    "        x = self.token_emb(x)\n",
    "        positions = self.pos_emb(x)\n",
    "        positions = positions * mask\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d0b5e32d-e1e9-4b7f-9b80-3c04c81383db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Note the input size (there is only one channel - intensity)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# these images... if you are using color images, your would\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# need to set the last dimension of the input_shape to -3-\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# above and this would carry over into this cell...\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m\u001b[43mx_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# This layer will just be passed a constant integer for\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# embedding (class token - see the ViT paper)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# input_class = keras.layers.Input(shape=(1,))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Patches\u001b[39;00m\n\u001b[1;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(embed_dim,\n\u001b[1;32m     18\u001b[0m                         kernel_size\u001b[38;5;241m=\u001b[39m(kernel,kernel),\n\u001b[1;32m     19\u001b[0m                         strides\u001b[38;5;241m=\u001b[39m(kernel,kernel))(tf\u001b[38;5;241m.\u001b[39mexpand_dims(y,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "kernel = 4\n",
    "num_heads = 6\n",
    "ff_dim = 512\n",
    "stack = 2\n",
    "\n",
    "# Note the input size (there is only one channel - intensity)\n",
    "# these images... if you are using color images, your would\n",
    "# need to set the last dimension of the input_shape to -3-\n",
    "# above and this would carry over into this cell...\n",
    "y = x = keras.layers.Input(shape=x_train.shape[1:])\n",
    "# This layer will just be passed a constant integer for\n",
    "# embedding (class token - see the ViT paper)\n",
    "# input_class = keras.layers.Input(shape=(1,))\n",
    "\n",
    "# Patches\n",
    "y = keras.layers.Conv2D(embed_dim,\n",
    "                        kernel_size=(kernel,kernel),\n",
    "                        strides=(kernel,kernel))(tf.expand_dims(y,-1))\n",
    "temp = y\n",
    "\n",
    "# Hybrid CNN\n",
    "# y = keras.layers.Conv2D(embed_dim*2,\n",
    "#                         kernel_size=(5, 5),\n",
    "#                         strides=2,\n",
    "#                         activation='gelu')(y)\n",
    "# y = keras.layers.Conv2D(embed_dim,\n",
    "#                         kernel_size=(5, 5),\n",
    "#                         strides=2,\n",
    "#                         activation='gelu')(y)\n",
    "\n",
    "# Flatten 2D arrangement to 1D arrangement of tokens\n",
    "y = keras.layers.Reshape((-1,embed_dim))(y)\n",
    "y = PositionEmbedding(y.shape[-2],embed_dim)(y)\n",
    "\n",
    "# Create class token\n",
    "c = keras.layers.Lambda(lambda x: tf.tile(tf.constant([[0]]),(tf.shape(x)[0],1)))(y)\n",
    "c = keras.layers.Embedding(input_dim=1,output_dim=embed_dim)(c)\n",
    "\n",
    "# Prepend class token\n",
    "y = keras.layers.Concatenate(axis=1)([c,y])\n",
    "\n",
    "for _ in range(stack):\n",
    "    y = TransformerBlock(embed_dim, num_heads, ff_dim)(y)[0]\n",
    "# Lambda layer is like the ViT the paper...\n",
    "# y = keras.layers.Lambda(lambda x: x[:,0,:])(y)\n",
    "# Use the following -instead- of Lambda is common in other\n",
    "# models I have seen... probably not important though\n",
    "y = keras.layers.GlobalAveragePooling1D()(y)\n",
    "# y = keras.layers.Dropout(0.1)(y)\n",
    "# y = keras.layers.Dense(embed_dim,activation='gelu')(y)\n",
    "#    y = keras.layers.Dropout(0.5)(y)\n",
    "# y = keras.layers.Dense(10)(y)\n",
    "\n",
    "model = keras.Model(x,y)\n",
    "# model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.0001),\n",
    "#               loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "# model.summary()\n",
    "masked_encoder = model\n",
    "# model = keras.Model(x,temp)\n",
    "# keras.utils.plot_model(model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71952c3f-d166-4d20-8688-548cdaee46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize and compile model\n",
    "n_tokens = len(tokenizer)\n",
    "embedding_size = 128\n",
    "stack = 5\n",
    "num_heads = 12\n",
    "memory_size = segment_size * 3\n",
    "\n",
    "y = x = keras.layers.Input((None,))\n",
    "y = MaskedTokenAndSinePositionEmbedding(input_dim=n_tokens,\n",
    "                                        output_dim=embedding_size)(y)\n",
    "for _ in range(stack):\n",
    "    y = GPTransformerBlock(embedding_size,\n",
    "                           num_heads,\n",
    "                           embedding_size*2,\n",
    "                           max_seq_len = max_length)(y)\n",
    "\n",
    "y = keras.layers.Dense(n_tokens)(y)\n",
    "\n",
    "model = keras.Model(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef5516-797f-4fe3-8927-78d5c83b3311",
   "metadata": {},
   "source": [
    "### Contrastive Model\n",
    "- Uses the Transformer as encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "747deaa4-5b56-41b4-bac3-9f2e3b136fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "def ContrastiveAccuracy(y_true, y_pred):\n",
    "    # return accuracy(y_true[tf.argmax(y_pred,-1)],\n",
    "    #                 y_pred)\n",
    "    acc_masked = accuracy(y_true, y_pred)\n",
    "    acc_unmasked = accuracy(y_true, tf.transpose(y_pred))\n",
    "    acc = (acc_masked + acc_unmasked) / 2.0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caeae7a6-e8d3-4ebc-8ec5-4448c3af22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        masked_encoder: tf.keras.Model,\n",
    "        unmasked_encoder: tf.keras.Model,\n",
    "        embed_dim: int = 512,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.masked_encoder = masked_encoder\n",
    "        self.unmasked_encoder = unmasked_encoder\n",
    "\n",
    "#         self.masked_encoder.trainable = False\n",
    "#         self.unmasked_encoder.trainable = False\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.W_masked = tf.keras.layers.Dense(\n",
    "            self.embed_dim,\n",
    "            # input_shape=self.masked_encoder.output_shape[-1],\n",
    "            use_bias=False)\n",
    "        self.W_unmasked = tf.keras.layers.Dense(\n",
    "            self.embed_dim,\n",
    "            # input_shape=self.unmasked_encoder.output_shape[-1],\n",
    "            use_bias=False)\n",
    "        self.t = self.add_weight(\n",
    "            name=\"Temperature\",\n",
    "            shape=None,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def compile(self, *args, **kwargs):\n",
    "        return super().compile(\n",
    "            *args,\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            **kwargs)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        n = tf.shape(data[0])[0]\n",
    "        y_true = tf.range(n)\n",
    "        y_pred = self(data, training=False)\n",
    "        loss_masked = self.compiled_loss(y_true, y_pred)\n",
    "        loss_unmasked = self.compiled_loss(y_true, tf.transpose(y_pred))\n",
    "        loss = (loss_masked + loss_unmasked) / 2.0\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        n = tf.shape(data[0])[0]\n",
    "        y_true = tf.range(n)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(data, training=True)\n",
    "            loss_masked = self.compiled_loss(y_true, y_pred)\n",
    "            loss_unmasked = self.compiled_loss(y_true, tf.transpose(y_pred))\n",
    "            loss = (loss_masked + loss_unmasked) / 2.0\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Get the images from input\n",
    "        masked_images, unmasked_images = inputs[0], inputs[1]\n",
    "\n",
    "        # Embed them using the encoders\n",
    "        masked_features = self.masked_encoder(masked_images)\n",
    "        unmasked_features = self.unmasked_encoder(unmasked_images)\n",
    "\n",
    "        # Joint multimodal embedding\n",
    "        masked_embeddings = self.W_masked(masked_features)\n",
    "        unmasked_embeddings = self.W_unmasked(unmasked_features)\n",
    "\n",
    "        # Normalize\n",
    "        masked_embeddings = masked_embeddings / tf.norm(masked_embeddings)\n",
    "        unmasked_embeddings = unmasked_embeddings / tf.norm(unmasked_embeddings)\n",
    "\n",
    "        logits = tf.tensordot(masked_embeddings, tf.transpose(unmasked_embeddings), axes=1) * tf.exp(self.t)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9c698ea-f3bc-4bd9-97a5-a518459c5188",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'masked_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cm \u001b[38;5;241m=\u001b[39m ContrastiveModel(\u001b[43mmasked_encoder\u001b[49m, unmasked_encoder)\n\u001b[1;32m      2\u001b[0m cm\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Set to true to debug\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masked_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "cm = ContrastiveModel(masked_encoder, unmasked_encoder)\n",
    "cm.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\n",
    "        ContrastiveAccuracy\n",
    "    ],\n",
    "    run_eagerly=False # Set to true to debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c18fcc-e423-4c65-99b2-c101df58d1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
