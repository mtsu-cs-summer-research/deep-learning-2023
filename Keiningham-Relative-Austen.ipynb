{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678fa6e-8869-4f54-a184-291ed4ba99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import GPT2Tokenizer to use\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce71511-476e-4614-b0fe-b2028bb9155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from IPython.display import display\n",
    "import sympy as sp\n",
    "import urllib\n",
    "sp.init_printing(use_latex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "#import os\n",
    "#os.environ['LD_LIBRARY_PATH']='/opt/conda/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261e075-0bad-4d61-903b-88ca974d58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative Mutli Head Attention from SirDavidLudwig's DeepDNA\n",
    "#https://github.com/DLii-Research/deep-dna/tree/master\n",
    "\n",
    "class RelativeMultiHeadAttention(keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, max_seq_len=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._max_seq_len = max_seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self._max_seq_len is None:\n",
    "            self._max_seq_len = input_shape[1]\n",
    "            assert self._max_seq_len is not None, \"RelativeMultiHeadAttention requires max_seq_len to be specified.\"\n",
    "        self._rel_embeds = self.add_weight(\"relative_embeddings\",\n",
    "                                           shape=(self._max_seq_len, self._key_dim),\n",
    "                                           initializer=\"glorot_uniform\", trainable=True)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_seq_len\": self._max_seq_len\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def _skew(self, QEr):\n",
    "        padded = tf.pad(QEr, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "        shape = tf.shape(padded)\n",
    "        reshaped = tf.reshape(padded, (shape[0], shape[1], shape[3], shape[2]))\n",
    "        return reshaped[:,:,1:,:]\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        # Note: Applying scalar multiply at the smaller end of einsum improves\n",
    "        # XLA performance, but may introduce slight numeric differences in\n",
    "        # the Transformer attention head.\n",
    "        query = tf.multiply(query, 1.0 / np.sqrt(float(self._key_dim)))\n",
    "\n",
    "        # Compute relative position encodings\n",
    "        rel_enc = self._skew(tf.einsum(\"acbd,ed->abce\", query, self._rel_embeds))\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "        # attention scores.\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores + rel_enc, attention_mask)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_scores_dropout = self._dropout_layer(attention_scores, training=training)\n",
    "\n",
    "        # `context_layer` = [B, T, N, H]\n",
    "        attention_output = tf.einsum(self._combine_equation, attention_scores_dropout, value)\n",
    "        return attention_output, attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bf688-56a2-4abf-b8e5-d8464dccea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transormer block setup\n",
    "# Initializes Relative Multie Head Attention, feed-forward network, layer normalization\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        #Using David's Relative Multi Head Attention\n",
    "        self.att = RelativeMultiHeadAttention(max_seq_len=max_seq_len, num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70993c4e-2095-41e1-9c58-f01f851beb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer, Relative Multi-Head-Attention, feed forward, and causal mask\n",
    "class GPTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, max_seq_len=None, **kwargs):\n",
    "        super(GPTransformerBlock, self).__init__(**kwargs)\n",
    "        #Using David's Relative Multi Head Attention\n",
    "        self.att = RelativeMultiHeadAttention(max_seq_len=max_seq_len, num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "             keras.layers.Dense(embed_dim), ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        y = x\n",
    "        y = self.layernorm1(y, training=training)\n",
    "        y = self.att(y, y,\n",
    "                     use_causal_mask=True,\n",
    "                     training=training)\n",
    "        y = self.dropout1(y, training=training)\n",
    "        x += y\n",
    "        y = x\n",
    "        y = self.layernorm2(y, training=training)\n",
    "        y = self.ffn(y, training=training)\n",
    "        y = self.dropout2(y, training=training)\n",
    "        return x + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597b470-1e2d-4d9f-86f5-669df0101937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional encoding for self-attention\n",
    "\n",
    "# Source pulled from KerasNLP: https://github.com/keras-team/keras-nlp/blob/v0.4.1/keras_nlp/layers/sine_position_encoding.py#L22\n",
    "class SinePositionEncoding(keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding layer.\n",
    "    This layer calculates the position encoding as a mix of sine and cosine\n",
    "    functions with geometrically increasing wavelengths. Defined and formulized\n",
    "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "    Takes as input an embedded token tensor. The input must have shape\n",
    "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
    "    positional encoding the same size as the embedded token tensor, which\n",
    "    can be added directly to the embedded token tensor.\n",
    "    This layer optionally accepts `tf.RaggedTensor`s as inputs to process\n",
    "    batches of sequences of different lengths. The one ragged dimension must be\n",
    "    the dimension that corresponds to the sequence, that is, the penultimate\n",
    "    dimension.\n",
    "    Args:\n",
    "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
    "            curves, as described in Attention is All You Need. Defaults to\n",
    "            10000.\n",
    "    Examples:\n",
    "    ```python\n",
    "    # create a simple embedding layer with sinusoidal positional encoding\n",
    "    seq_len = 100\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 32\n",
    "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_dim\n",
    "    )(inputs)\n",
    "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
    "    outputs = embedding + positional_encoding\n",
    "    ```\n",
    "    References:\n",
    "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_wavelength=10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_wavelength = max_wavelength\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
    "        # with other layers.\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            bounding_shape = inputs.bounding_shape()\n",
    "            position_embeddings = (\n",
    "                self._compute_trim_and_broadcast_position_embeddings(\n",
    "                    bounding_shape,\n",
    "                )\n",
    "            )\n",
    "            # then apply row lengths to recreate the same ragged shape as inputs\n",
    "            return tf.RaggedTensor.from_tensor(\n",
    "                position_embeddings,\n",
    "                inputs.nested_row_lengths(),\n",
    "            )\n",
    "        else:\n",
    "            return self._compute_trim_and_broadcast_position_embeddings(\n",
    "                tf.shape(inputs),\n",
    "            )\n",
    "\n",
    "    def _compute_trim_and_broadcast_position_embeddings(self, shape):\n",
    "        seq_length = shape[-2]\n",
    "        hidden_size = shape[-1]\n",
    "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
    "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
    "        timescales = tf.pow(\n",
    "            min_freq,\n",
    "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
    "            / tf.cast(hidden_size, self.compute_dtype),\n",
    "        )\n",
    "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
    "        # even indices are sine, odd are cosine\n",
    "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
    "        sin_mask = 1 - cos_mask\n",
    "        # embedding shape is [seq_length, hidden_size]\n",
    "        positional_encodings = (\n",
    "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
    "        )\n",
    "\n",
    "        return tf.broadcast_to(positional_encodings, shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_wavelength\": self.max_wavelength,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fe920-63e0-4b15-b1af-3a3f466ba419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Token and position embedding to allow the model to differentiate token positions, for self-attention\n",
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, input_dim, output_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "                                              output_dim=output_dim,\n",
    "                                              mask_zero=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c07eef-d0a6-43d9-ae35-22f4c73a3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sine position embeddings, for self-attention\n",
    "class MaskedTokenAndSinePositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, max_wavelength=10000,**kwargs):\n",
    "        super(MaskedTokenAndSinePositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=input_dim,\n",
    "                                                output_dim=output_dim,\n",
    "                                                mask_zero=True)\n",
    "        self.pos_emb = SinePositionEncoding(max_wavelength=max_wavelength)\n",
    "\n",
    "    def call(self, x):\n",
    "        mask = tf.expand_dims(tf.sign(x),-1)\n",
    "        x = self.token_emb(x)\n",
    "        positions = self.pos_emb(x)\n",
    "        positions = positions * mask\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6397c9-897a-4e3e-afc9-862aa9dd1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom masked loss/accuracy functions\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67db5ca-183c-496d-b4fd-f0386e7f87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/Austen.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540cd38-8382-4e0a-9eaa-003bf66d1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 20\n",
    "model_length = 10*segment_size\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261339d7-2004-486d-8816-473cff3b1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "text = []\n",
    "j = 0\n",
    "for i in range(len(lines)-1):\n",
    "    if lines[i] == '':\n",
    "        line = ' '.join(lines[j:i])\n",
    "        if line != '':\n",
    "            text = text + [line[k:k+model_length-2] for k in range(0, len(line), model_length-2)]\n",
    "            # text = text + [line]\n",
    "        j = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b519a8b-584e-4ef7-bebf-3f57d96251f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92df22-3081-44b9-9d9a-4371d6f1f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73e706-3f11-401e-af31-0eb2ded93ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset? - All of the data will take some time...\n",
    "n_seq = len(text)\n",
    "n_seq = 1000\n",
    "split_point = int(n_seq * 0.8)\n",
    "text = text[:n_seq]\n",
    "np.random.shuffle(text) # In-place modification\n",
    "max_length = np.max([len(i) for i in text])+2 # Start+stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b45aa-bc58-4293-8930-bd87f072718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode / Decode tokens using built-in functions\n",
    "\n",
    "def encode_seq(x, tokenizer, max_length=0):\n",
    "    #String to token IDs\n",
    "    #Using end of text token as padding\n",
    "    encoded = tokenizer.encode(x, max_length=max_length, truncation=True)\n",
    "    padding = encoded + [50256 for i in range (max_length - len(encoded))]\n",
    "    return padding\n",
    "\n",
    "def decode_seq(x, tokenizer):\n",
    "    #Token IDs to string\n",
    "    remove_padding = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 50256: #end of text\n",
    "            break\n",
    "        remove_padding.append(x[i])\n",
    "    return tokenizer.decode(remove_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1324d-7317-40fc-a716-4cc111ab27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = encode_seq(text[0], tokenizer, max_length)\n",
    "print(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe7ed4-ae14-43f4-b8aa-e99fb1ca13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031ef85-c9ae-4899-a0c2-a961e0dceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_temp = decode_seq(temp, tokenizer)\n",
    "print(decoded_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92016fc9-12da-465c-9229-11ae5f8152ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, max_length, batch_size=32, **kwargs):\n",
    "        super(DataGenerator, self).__init__(**kwargs)\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data # a handle only...\n",
    "        self.indices = np.arange(self.data[0].shape[0])\n",
    "        self.max_length = max_length\n",
    "        self.idx = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'The number of batches per epoch'\n",
    "        return int(np.floor(self.data[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one (enumerated) batch of data'\n",
    "        # Generate indices for a batch and grab batch\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return self.__data_generation(indices)\n",
    "\n",
    "    def __data_generation(self, ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Generate data\n",
    "        x = tf.convert_to_tensor(self.data[0][ids],dtype=tf.int32)\n",
    "        y = tf.convert_to_tensor(self.data[1][ids],dtype=tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        return np.random.shuffle(self.indices)\n",
    "\n",
    "    ## Needed for TF Dataset conversion...\n",
    "    def output_signature(self):\n",
    "        sig = self[0]\n",
    "        return (tf.TensorSpec.from_tensor(sig[0]),\n",
    "                tf.TensorSpec.from_tensor(sig[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f68a2c-59ee-4ef7-a9e5-fc76fd5a1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode in batches\n",
    "X = np.vstack([encode_seq(x,tokenizer,max_length + (segment_size - (max_length-1) % segment_size)) for x in text])\n",
    "training = DataGenerator((X[:split_point,:-1],\n",
    "                          X[:split_point,1:]),model_length,batch_size)\n",
    "validation = DataGenerator((X[split_point:,:-1],\n",
    "                            X[split_point:,1:]),model_length,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d725a0c-d64d-444f-ab7f-f62972001c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268033c5-de99-49b5-a6c4-5d985aa7e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_length = len(temp)\n",
    "temp = np.array([temp_length+1 for i in temp])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42fa26-6fb8-48f2-826f-c3955345c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.random.randint(i) for i in temp])-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32957d5c-4627-41ed-9e6d-e41a7d7eab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_batch = training[0]\n",
    "my_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2adcec-fd5d-4339-8805-61d991088907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalize and compile model\n",
    "n_tokens = len(tokenizer)\n",
    "embedding_size = 128\n",
    "stack = 5\n",
    "num_heads = 12\n",
    "memory_size = segment_size * 3\n",
    "\n",
    "y = x = keras.layers.Input((None,))\n",
    "y = MaskedTokenAndSinePositionEmbedding(input_dim=n_tokens,\n",
    "                                        output_dim=embedding_size)(y)\n",
    "for _ in range(stack):\n",
    "    y = GPTransformerBlock(embedding_size,\n",
    "                           num_heads,\n",
    "                           embedding_size*2,\n",
    "                           max_seq_len = max_length)(y)\n",
    "\n",
    "y = keras.layers.Dense(n_tokens)(y)\n",
    "\n",
    "model = keras.Model(x,y)\n",
    "#model.compile(loss=MaskedSparseCategoricalCrossentropy,\n",
    "#              optimizer=keras.optimizers.Adam(),\n",
    "#              metrics=MaskedSparseCategoricalAccuracy)\n",
    "#model.summary()\n",
    "#keras.utils.plot_model(model,show_shapes=True,expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d7a42-34cf-4675-9c93-688f6f9a90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O https://s-stem-data.nyc3.digitaloceanspaces.com/Relative-Weights-Keiningham/Relative-Weights-Keiningham.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1b2ce-c822-4e06-b884-a315ea9acfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xf Relative-Weights-Keiningham.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff3f08-c7a3-46ff-bee4-926001c280c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Relative-Weights-Keiningham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7625e85-f2a0-4ce4-8f6a-625c5aa09f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should we see?\n",
    "i = 0\n",
    "print('Input:', training[0][0][i])\n",
    "print('Output:', training[0][1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a46483-4524-43ab-92cb-6c763c8504df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-forcing\n",
    "result = model(training[0][0][i:i+1]).numpy()\n",
    "result.argmax(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699df111-02c0-46e4-91e4-db78c357878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_seq(result.argmax(-1)[0],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224485d-9eec-449a-84e8-f5b6cbdf50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much agreement really? - remember masked tokens don't count\n",
    "MaskedSparseCategoricalAccuracy(training[0][1][i:i+1],result).numpy()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26c7db-61b2-4369-a347-88d8615fc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_token(x):\n",
    "    x = np.cumsum(x)\n",
    "    return np.argmax(x > np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c8ae8-8738-46af-9803-4885c7848632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 1\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 6\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=50256) #Courtesy of Parker Daniels \n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 50256:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca94d41-e589-4f00-a7fc-d527ccba4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "i = 2\n",
    "data = training[0][0][i:i+1]\n",
    "prompt = 4\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=50256) #Courtesy of Parker Daniels\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 50256:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3655a7-0dee-447a-9ea7-e83bf9d35c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off teacher forcing...\n",
    "# Prompt (needs to be at least 1 - the start token?...)\n",
    "sample = \"It is a truth universally acknowledged\"\n",
    "data = np.array(encode_seq(sample, tokenizer, max_length))\n",
    "data = data.reshape(1,-1)\n",
    "prompt = len(sample.split())\n",
    "tokens = np.full(data.shape,dtype=np.int32,fill_value=50256) #Courtesy of Parker Daniels\n",
    "tokens[0,0:prompt] = data[:,0:prompt]\n",
    "\n",
    "print(\"Original:\")\n",
    "print(decode_seq(data[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Prompt:\")\n",
    "print(decode_seq(tokens[0],tokenizer))\n",
    "print()\n",
    "\n",
    "print(\"Decoding:\")\n",
    "print(decode_seq(tokens[0],tokenizer),end='')\n",
    "for x in range(prompt,data.shape[1]-1):\n",
    "    probabilities = keras.activations.softmax(model(tokens)).numpy()[0,x-1]\n",
    "    # Most likely token...\n",
    "    result = probabilities.argmax(-1)\n",
    "    # Sampled token...\n",
    "    result = np.apply_along_axis(select_token, -1, probabilities)\n",
    "    tokens[0,x] = result\n",
    "    if result == 50256:\n",
    "        break # Stop token found!\n",
    "    print(decode_seq(tokens[0,x:x+1],tokenizer),end='')\n",
    "print()\n",
    "print()\n",
    "\n",
    "result = model(tokens).numpy()\n",
    "print(\"Remodeled:\")\n",
    "print(decode_seq(result.argmax(-1)[0],tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0408940-6498-413e-b5a5-376cd109080c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
